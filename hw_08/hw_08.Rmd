---
title: "Eighth Week: Text Analysis in R"
subtitle: "To be, or not to be"
author: "Mina Moosavifar - 93106788"
date: "`r Sys.time()`"
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---

<div align="center">
<img  src="images/dickens1_1.png"  align = 'center'>
</div>

> <p dir="RTL"> 
با استفاده از بسته gutenberg داده های لازم را به دست آورید و به سوالات زیر پاسخ دهید.
</p>

***
<p dir="RTL">
بارگزاری داده ها و کتابخانه ها:
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
library(gutenbergr)
library(tidyverse)
library(stringr)
library(tidytext)
```

***

<p dir="RTL">
۱. چارلز دیکنز نویسنده معروف انگلیسی بالغ بر چهارده رمان (چهارده و نیم) نوشته است. متن تمامی کتاب های او را دانلود کنید و سپس بیست لغت برتر استفاده شده را به صورت یک نمودار ستونی نمایش دهید. (طبیعتا باید ابتدا متن را پاکسازی کرده و stopping words را حذف نمایید تا به کلماتی که بار معنایی مشخصی منتقل می کنند برسید.)
</p>

<p dir="RTL">
ابتدا تمامی کتاب های چارلز دیکنز را با کمک
gutenberg 
بدست می آوریم. سپس ۱۵ رمان برتر نویسنده را انتخاب می کنیم. سپس تک تک کتاب را دانلود کرده و آن ها را در 
book_texts 
ذخیره می کنیم. (در سایر بخش های تمرین از این داده برای تحلیل متن بهره می بریم.)
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
library(highcharter)

dickens_list <- gutenberg_works(author == "Dickens, Charles")
dickens_list <- dickens_list[c(2, 3, 4, 12, 13, 14, 15, 19, 23, 27, 32, 33, 34, 35, 39), ]
dickens_list <- dickens_list %>% select(gutenberg_id, title)
```

```{r, message=FALSE, warning=FALSE, comment=NA, eval=FALSE}
books_texts = list()

# downloading all books
for( i in 1:nrow(dickens_list)){
  book = gutenberg_download(dickens_list[i, 1, 1])
  books_texts[[i]] = book
}
saveRDS(books_texts, file="data/booktext.rds")
```

<p dir="RTL">
برای بدست آوردن کلماتی که بیشترین تکرار را داشته اند، ابتدا تمامی متن را به 
lowercase 
تبدیل می کنیم. سپس نشانه گذاری را حذف می کنیم و متن را به کلمات آن تقسیم می کنیم. سپس فرآیند فوق را به ازای تمامی رمان ها انجام داده و همه را در یک دیتا فریم قرار می دهیم. در نهایت از این دیتا 
stopwords،
اعداد و تک حرف را حذف می کنیم. در نهایت تکرار کلمات را بدست می آوریم.
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
books_texts = readRDS(file="data/booktext.rds")
# using books
all_books = list()
for( i in 1:nrow(dickens_list)){
  book = books_texts[[i]]
  wbook = book %>% mutate(text = str_to_lower(text)) %>% 
    select(text) %>% 
    str_replace_all("\"","") %>% 
    str_replace_all("[[:punct:]]","") %>% str_split(pattern = "\\s") %>% 
    unlist() %>% as.data.frame(stringsAsFactors = F)
    if(i == 1){
      all_books = wbook
    } else {
      all_books = bind_rows(all_books, wbook)
    }
}  
  

all_books <- all_books %>% table() %>% 
  as.data.frame(stringsAsFactors = F)
  
colnames(all_books) = c("word","count")

all_books <- all_books %>%
    filter(!word %in% stop_words$word) %>% 
    filter(str_length(word)>1) %>% 
    filter(!str_detect(word,"\\d")) %>%
    arrange(desc(count))

top_words <- all_books %>% slice(1:20)

top_words %>% 
  hchart(type = "column",hcaes(x = word, y = count, color = count)) %>% 
  hc_title(text = "Most repeated words", style = list(fontWeight = "bold")) %>% 
  hc_add_theme(hc_theme_sandsignika())
```


***

<p dir="RTL">
۲. ابر لغات ۲۰۰ کلمه پرتکرار در رمان های چارلز دیکنز را رسم نمایید. این کار را با بسته wordcloud2 انجام دهید. برای دانلود می توانید به لینک زیر مراجعه کنید.
</p>

https://github.com/Lchiffon/wordcloud2

<p dir="RTL">
 با استفاده از عکسی که در ابتدا متن آمده ابر لغاتی مانند شکل زیر رسم کنید. (راهنمایی: از ورودی figpath در دستور wordcloud2 استفاده نمایید.مثالی در زیر آورده شده است.)
</p>

<div align="center">
<img  src="images/tag-word-cloud-Che-Guevara.jpg"  align = 'center'>
</div>

<p dir="RTL">
ابتدا بسته ی 
wordcloud2 
را نصب می کنیم، سپس سعی می کنیم که ابرلغات را رسم کنیم.
</p>

```{r, message=FALSE, warning=FALSE, comment=NA, eval=FALSE}
library(wordcloud2)

word_fig <- all_books %>% slice(1:200) %>% select(word, freq = count)

wordcloud2(word_fig , figPath = "images/dickens1_1.png" , size = 0.17, color = "black")
```
<div align="center">
<img  src="images/A2a.png"  align = 'center'>
</div>

<p dir="RTL">
از آنجایی که ابرلغات بالا واضح نبود، ابر لغاتی به شکل صورت انسان نیز در ادامه آمده است.
</p>
<div align="center">
<img  src="images/A2.png"  align = 'center'>
</div>


***

<p dir="RTL">
۳. اسم پنج شخصیت اصلی در هر رمان دیکنز را استخراج کنید و با نموداری تعداد دفعات تکرار شده بر حسب رمان را رسم نمایید. (مانند مثال کلاس در رسم اسامی شخصیت ها در سری هر پاتر)
</p>

<p dir="RTL">
برای بدست آوردن شخصیت ها، ابتدا نشانه گذاری ها را از متن کتاب حذف کرده و سپس آن را تبدیل به کلمات می کنیم. سپس 
stopwords، 
اعداد، تک حرف، کلماتی که با حرف کوچک شروع می شوند و کلماتی که هم با حروف بزرگ و هم حروف کوچک قرار دارند را حدف می کنیم. سپس فرآیند فوق را به ازای تمامی رمان ها انجام می دهیم. سپس ۵ شخصیت پرتکرار هر کتاب را که بیشترین درصد حضور در متن را دارند را به عنوان شخصیت اصلی قرار می دهیم.
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
character_list = list()
for( i in 1:nrow(dickens_list)){
  book = books_texts[[i]]
  wbook = book %>% 
    str_replace_all("\"","") %>% 
    str_replace_all("[[:punct:]]","") %>% 
    str_split(pattern = "\\s") %>% 
    unlist() %>% 
    table() %>% 
    as.data.frame(stringsAsFactors = F)
  
  colnames(wbook) = c("word","count")
  wbook = wbook %>%
    filter(!str_to_lower(word) %in% stop_words$word) %>% 
    filter(str_length(word)>1) %>% 
    filter(!str_detect(word,"\\d")) %>%
    arrange(desc(count)) %>% 
    mutate(proper = !word %in% str_to_lower(word)) %>% 
    mutate(Book = dickens_list[i,2,1]) %>% 
    filter(proper == TRUE)
  character_list[[i]] = wbook
}

characters = bind_rows(character_list)
    
top_characters <- characters %>% 
  group_by(Book) %>% 
  mutate(percent = round(100*count/sum(count))) %>% 
  arrange(desc(percent)) %>% 
  mutate(rank = row_number() %>% as.integer()) %>% 
  filter(rank < 6) %>%
  rename(name = word)

top_characters %>% 
  hchart("column", hcaes(x = Book, y = percent, group = name)) %>% 
  hc_add_theme(hc_theme_google())
```

***

<p dir="RTL">
۴.  در بسته tidytext داده ایی به نام sentiments وجود دارد که فضای احساسی لغات را مشخص می نماید. با استفاده از این داده نمودار ۲۰ لغت برتر negative و ۲۰ لغت برتر positive را در کنار هم رسم نمایید. با استفاده از این نمودار فضای حاکم بر داستان چگونه ارزیابی می کنید؟ (به طور مثال برای کتاب داستان دو شهر فضای احساسی داستان به ترتیب تکرر در نمودار زیر قابل مشاهده است.)
</p>

<div align="center">
<img  src="images/sentiments.png"  align = 'center'>
</div>

<p dir="RTL">
ابتدا کتاب ها را تبدیل به لغات کرده و سپس همه ی آن ها را در کنار یکدیگر قرار می دهیم. سپس 
stopwords 
را از داده حذف می کنیم و سپس برای هر کتاب، اشتراکات با لغات
bing 
را بدست می آوریم و تعداد تکرار آن ها را می شماریم. در نهایت نمودار ۲۰ لغت پر تکرار مثبت و منفی هر کتاب را رسم می کنیم.
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
tidy_data = list()
for( i in 1:nrow(dickens_list)){
  book = books_texts[[i]]
  wbook = book %>% mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, regex("chapter [\\divxlc]+", 
                                                 ignore_case = TRUE)))) %>%
    unnest_tokens(word, text) %>% 
    mutate(book = dickens_list[i,2,1])
  tidy_data[[i]] = wbook
}
tidy_books = bind_rows(tidy_data)
tidy_books  <- tidy_books %>% anti_join(stop_words)

bing_word_counts <- tidy_books %>%
  group_by(book) %>% 
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE)

for( i in 1:nrow(dickens_list)){
  plot_name = paste("Sentiment in ", dickens_list[i,2,1], sep=" ")
  p <- bing_word_counts %>%
    filter(book == dickens_list[i,2,1]) %>% 
    group_by(sentiment) %>%
    top_n(20) %>%
    mutate(word = reorder(word, n)) %>%
    ggplot(aes(word, n, fill = sentiment)) +
    geom_col(show.legend = FALSE) +
    facet_grid(~sentiment, scales="free_y") +
    labs(y = plot_name,
         x = NULL) +
    coord_flip()
  print(p)
}
```

***

<p dir="RTL">
۵. متن داستان بینوایان را به ۲۰۰ قسمت مساوی تقسیم کنید. برای هر قسمت تعداد لغات positive و negative را حساب کنید و سپس این دو سری زمانی را در کنار هم برای مشاهده فضای احساسی داستان رسم نمایید.
</p>

<p dir="RTL">
ابتدا کتاب بی نوایان را از 
gutenberg 
دانلود می کنیم.
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
hugo_list <- gutenberg_works(author == "Hugo, Victor")
hugo_list <- hugo_list[c(9:13), ] 
```

<p dir="RTL">
سپس همانند سوال یک این کتاب ها را برای استفاده ی آینده ذخیره می کنیم، تا هر بار نیازی به دانلود کتاب ها نباشد.
</p>

```{r, message=FALSE, warning=FALSE, comment=NA, eval=FALSE}
miserable_texts = list()
# downloading all books
for( i in 1:nrow(hugo_list)){
  book = gutenberg_download(hugo_list[i, 1, 1])
  miserable_texts[[i]] = book
}
saveRDS(miserable_texts, file="data/miserable.rds")

```

<p dir="RTL">
ابتدا کتاب ها را تبدیل به لغات کرده و سپس همه ی آن ها را در کنار یکدیگر قرار می دهیم. سپس 
stopwords 
را از داده حذف می کنیم. سپس به کمک تابع 
split 
داده ها را به ۲۰۰ قسمت با طول برابر با نسبت تعداد ردیف ها به ۲۰۰ تقسیم می کنیم. سپس برای هر دسته اشتراکات با
bing 
را بدست می آوریم و تعداد لغات مثبت و منفی را برای هر دسته حساب می کنیم و در نهایت نمودار را می کشیم.
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
miserable_texts = readRDS(file="data/miserable.rds")

tidy_data = list()
for( i in 1:nrow(hugo_list)){
  book = miserable_texts[[i]]
  wbook = book %>% mutate(linenumber = row_number(),
                          chapter = cumsum(str_detect(text, regex("chapter [\\divxlc]+", 
                                                                  ignore_case = TRUE)))) %>%
    unnest_tokens(word, text) %>% 
    mutate(book = hugo_list[i,2,1])
  tidy_data[[i]] = wbook
}
miserable_books = bind_rows(tidy_data)
miserabley_books  <- miserable_books %>% anti_join(stop_words)

n <- 200
nr <- nrow(miserabley_books)
miserabley_books_part <- split(miserabley_books, rep(1:ceiling(nr/n), each=n, length.out=nr))

books = miserabley_books_part[[1]]
mis_bing_word_counts <- books %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sentiment, sort = TRUE)
pos = c(mis_bing_word_counts %>% filter(sentiment == "positive") %>% count())
neg = c(mis_bing_word_counts %>% filter(sentiment == "negative") %>% count())
mis_atmo = data.frame(pos , neg, stringsAsFactors=FALSE)
colnames(mis_atmo) = c("pos", "neg")

for( i in 2:200){
  books = miserabley_books_part[[i]]
  mis_bing_word_counts <- books %>%
    inner_join(get_sentiments("bing")) %>%
    count(word, sentiment, sort = TRUE)
  pos = c(mis_bing_word_counts %>% filter(sentiment == "positive") %>% count())
  neg = c(mis_bing_word_counts %>% filter(sentiment == "negative") %>% count())
  mis_atmo1 = data.frame(pos , neg, stringsAsFactors=FALSE)
  colnames(mis_atmo1) = c("pos", "neg")
  mis_atmo = bind_rows(mis_atmo, mis_atmo1)
}

mis_atmo <- mis_atmo %>% mutate(part = row_number())

hc <- highchart() %>% 
  hc_xAxis(categories = mis_atmo$part) %>% 
  hc_add_series(name = "Postive", data = mis_atmo$pos) %>% 
  hc_add_series(name = "Negative", data = mis_atmo$neg) %>% 
  hc_title(text = "Emotional Atmosphere of Les Misérables", style = list(fontWeight = "bold")) %>% 
  hc_add_theme(hc_theme_elementary())

hc
```

<p dir="RTL">
همانطور که مشاهده می کنیم، در اکثر داستان، فضای غمگین حکمفرما بوده است و در موارد کمی، فضای مثبت بیشتری نسبت به فضای منفی داشته ایم.
</p>

***

<p dir="RTL">
۶. ابتدا ترکیبات دوتایی کلماتی که پشت سر هم می آیند را استخراج کنید و سپس نمودار ۳۰ جفت لغت پرتکرار را رسم نمایید.
</p>

<p dir="RTL">
برای بدست آوردن لغات پشت سر هم، تبدیل جملات به کلمات را به کمک تابع
unnest_tokens 
انجام می دهیم، با این تفاوت که این بار توکن را برابر با 
ngram 
قرار داده و برای بدست آوردن هر دو کلمه ی متوالی،
n 
را برابر با ۲ قرار می دهیم. سپس از آنجایی که بسیاری از داده ها، ترکیباتی از 
stopwords 
هستند، هر ترکیب را به کلمات آن شکسته و 
stopwords 
آن را حذف می کنیم، در نهایت دوباره لغات را به یکدیگر اضافه کرده و نمودار ترکیباتی که بیشترین تکرار را داشته اند را رسم می کنیم.(البته نمودار را هم برای ترکیبات با 
stopwords 
و هم بدون 
stopwords 
رسم می کنیم.)
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
tidy_data = list()
for( i in 1:nrow(dickens_list)){
  book = books_texts[[i]]
  wbook = book %>% mutate(linenumber = row_number(),
                          chapter = cumsum(str_detect(text, regex("chapter [\\divxlc]+", 
                                                                  ignore_case = TRUE)))) %>%
    filter(text != "") %>% 
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
    mutate(book = dickens_list[i,2,1])
  tidy_data[[i]] = wbook
}

tidy_colocation_books = bind_rows(tidy_data)

tidy_colocation_separated <- tidy_colocation_books %>%
  separate(bigram, c("first_word", "second_word"), sep = " ")

untidy_colocation_count <- tidy_colocation_separated %>% 
  select(-gutenberg_id, -linenumber) %>% 
  count(first_word, second_word, sort = TRUE) %>% 
  mutate(collocation = paste(first_word, second_word, sep=" "))

top_30_collocation <- untidy_colocation_count %>% slice(1:30) %>% 
  rename(count = n)

top_30_collocation %>% 
  hchart(type = "column",hcaes(x = collocation, y = count, color = count)) %>% 
  hc_title(text = "Most repeated collocations with stopwords", style = list(fontWeight = "bold")) %>% 
  hc_add_theme(hc_theme_sandsignika())

# removing stop words
tidy_colocation_filtered <- tidy_colocation_separated %>%
  filter(!first_word %in% stop_words$word) %>%
  filter(!second_word %in% stop_words$word)

tidy_colocation_count <- tidy_colocation_filtered %>% 
  count(first_word, second_word, sort = TRUE)

tidy_colocation_count <- tidy_colocation_count[-c(1, 2), ]

tidy_colocation_count <- tidy_colocation_count %>%
  mutate(collocation = paste(first_word, second_word, sep=" "))

top_30_collocation <- tidy_colocation_count %>% slice(1:30) %>% 
  rename(count = n)

top_30_collocation %>% 
  hchart(type = "column",hcaes(x = collocation, y = count, color = count)) %>% 
  hc_title(text = "Most repeated collocations without stopwords", style = list(fontWeight = "bold")) %>% 
  hc_add_theme(hc_theme_sandsignika())
```

***

<p dir="RTL">
۷. جفت کلماتی که با she و یا he آغاز می شوند را استخراج کنید. بیست فعل پرتکراری که زنان و مردان در داستان های دیکنز انجام می دهند را استخراج کنید و نمودار آن را رسم نمایید.
</p>

<p dir="RTL">
برای این سوال از داده ای که در سوال قبل بدست آوریم اما 
stopwords 
آن را حذف نکردیم استفاده می کنیم. برای بدست آوردن ترکیبات
he 
و 
she 
عباراتی را که کلمه اول آن ها، برابر با کلمات فوق است را فیلتر می کنیم و سپس بر اساس جنسیت گروه بندی کرده و ۲۰ ترکیب پر تکرار هر گروه را نمایش می دهیم.(البته این نمودار را یک بار با حذف 
stopwords 
فعل ها و یک بار بدون آن رسم می کنیم.) همانطور که مشاهده می کنیم، نموداری که 
stopwords 
آن حذف شده است، فعل های با معنی تری دارد.
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
she_or_he <- untidy_colocation_count %>% filter(first_word == "she" | first_word == "he")

she_or_he <- she_or_he %>% group_by(first_word) %>% arrange(desc(n)) %>% top_n(20, wt = n)

# with stopwords
she_or_he %>% ungroup() %>% filter(first_word == "she") %>% rename(count = n) %>% 
  hchart(type = "column",hcaes(x = second_word, y = count, color = count)) %>% 
  hc_title(text = "Most repeated female verbs with stopwords", style = list(fontWeight = "bold")) %>% 
  hc_add_theme(hc_theme_flat())

she_or_he %>% ungroup() %>% filter(first_word == "he") %>% rename(count = n) %>% 
  hchart(type = "column",hcaes(x = second_word, y = count, color = count)) %>% 
  hc_title(text = "Most repeated male verbs with stopwords", style = list(fontWeight = "bold")) %>% 
  hc_add_theme(hc_theme_darkunica())

# without stopwords
she_or_he <- untidy_colocation_count %>% filter(first_word == "she" | first_word == "he") %>% 
  filter(!second_word %in% stop_words$word) %>% 
  group_by(first_word) %>% arrange(desc(n)) %>% top_n(20, wt = n)

she_or_he %>% ungroup() %>% filter(first_word == "she") %>% rename(count = n) %>% 
  hchart(type = "column",hcaes(x = second_word, y = count, color = count)) %>% 
  hc_title(text = "Most repeated female verbs without stopwords", style = list(fontWeight = "bold")) %>% 
  hc_add_theme(hc_theme_flat())

she_or_he %>% ungroup() %>% filter(first_word == "he") %>% rename(count = n) %>% 
  hchart(type = "column",hcaes(x = second_word, y = count, color = count)) %>% 
  hc_title(text = "Most repeated male verbs without stopwords", style = list(fontWeight = "bold")) %>% 
  hc_add_theme(hc_theme_darkunica())

```


***

<p dir="RTL">
۸. برای کتاب های دیکنز ابتدا هر فصل را جدا کنید. سپی برای هر فصل 
1-gram, 2-gram
را استخراج کنید. آیا توزیع  N-gram
در کارهای دیکنز یکسان است؟ با رسم نمودار هم این موضوع را بررسی کنید.
</p>

<p dir="RTL">
برای حل این سوال، ابتدا هر کتاب را از طریق تشخیص 
regex 'chapter [\\divxlc]+' 
به فصل ها تقسیم می کنیم. سپس هر کتاب را بر اساس فصل گروه بندی می کنیم و 
1-gram 
و 
2-gram 
برای داده ها محاسبه می کنیم. سپس برای بدست آوردن توزیع 
n-gram 
از مفهومی به نام
term frequency 
استفاده می کنیم که میزان اهمیت کلمات و عبارات را در اسناد نمایش می دهد. سپس از قانون
Zipf's 
استفاده می کنیم، این قانون نشان می دهد که فرکانس عبارت با رتبه ی آن رابطه ی عکس دارد. سپس برای نمایش رابطه ی عکس این دو متغیر نمودار را بر حسب 
log 
رسم می کنیم.
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
uni_data = list()
bi_data = list()
for( i in 1:nrow(dickens_list)){
  book = books_texts[[i]]
  wbook = book %>% mutate(chapter = cumsum(str_detect(text, regex("chapter [\\divxlc]+", ignore_case = TRUE)))) %>%
    filter(text != "") %>% 
    group_by(chapter)
  
  bi = wbook %>% unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
    separate(bigram, c("first_word", "second_word"), sep = " ") %>%
    count(first_word, second_word, sort = TRUE) %>% 
    mutate(collocation = paste(first_word, second_word, sep=" "), 
           chapter_words = sum(n)) %>% 
    mutate(index = row_number(), tf = n/chapter_words) %>% 
    mutate(book = dickens_list[i,2,1])
  bi_data[[i]] = bi
  
  uni = wbook %>% unnest_tokens(unigram, text, token = "ngrams", n = 1) %>% 
    count(unigram , sort = TRUE) %>% 
    mutate(chapter_words = sum(n), index = row_number(),
           tf = n/chapter_words) %>% 
    mutate(book = dickens_list[i,2,1])
  uni_data[[i]] = uni
}

unigram_data_all = bind_rows(uni_data) %>% as.data.frame(stringsAsFactors = F)
unigram_data_all <- unigram_data_all %>% ungroup() %>% 
  group_by(book, chapter)

co = lm(log10(tf) ~ log10(index), data = unigram_data_all)
p = unigram_data_all %>% 
  ggplot(aes(index, tf, color = book)) + 
  geom_abline(intercept = co$coefficients[1], slope = co$coefficients[2], color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 1) + 
  ylab("term frequency") +
  ggtitle("1-gram distribution for Charles Dickens") +
  scale_x_log10() +
  scale_y_log10()
p

bigram_data_all = bind_rows(bi_data) %>% as.data.frame(stringsAsFactors = F)
bigram_data_all <- bigram_data_all %>% ungroup() %>% 
  group_by(book, chapter)

co = lm(log10(tf) ~ log10(index), data = bigram_data_all)
p = bigram_data_all %>% 
  ggplot(aes(index, tf, color = book)) + 
  geom_abline(intercept = co$coefficients[1], slope = co$coefficients[2], color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 1) + 
  ylab("term frequency") +
  ggtitle("2-gram distribution for Charles Dickens") +
  scale_x_log10() +
  scale_y_log10()
p
```

<p dir="RTL">
همانطور که در نمودار مشاهده می کنیم، آثار چارلز دیکنز عمدتا با یکدیگر شباهت زیادی دارند و رابطه ی رتبه و فرکانس شیب منفی دارد. البته مشاهده می کنیم که کتاب
Martin Chuzzlewit
با سایر کتاب ها تفاوت قابل توجهی دارد.
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
kruskal.test(book ~ tf, unigram_data_all)

kruskal.test(book ~ tf, bigram_data_all)
```

<p dir="RTL">
برای تست تفاوت میان کتاب های مختلف، از تست
kruskal 
استفاده می کنیم، اما همانطور که در نمودار نیز مشاهده کردیم، کتاب ها کاملا با یکدیگر مطابق نبوده و در نتیجه این موضوع باعث می شود که فرض صفر ما که کتاب ها از یک توزیع آمده اند، باطل شود.
</p>

***

<p dir="RTL"> 
۹. برای آثار ارنست همینگوی نیز تمرین ۸ را تکرار کنید. آیا بین آثار توزیع n-grams در بین آثار این دو نویسنده یکسان است؟
</p>

<p dir="RTL"> 
از آنجایی که آثار همینگوی در 
gutenburg 
قرار ندارد، از آثار 
Jane Austen
استفاده می کنیم. سپس فرآیند سوال قبل را تکرار می کنیم.
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
woolf_list <- gutenberg_works(author == "Austen, Jane")
woolf_list <- woolf_list[-c(6, 9, 10), ]
```

```{r, message=FALSE, warning=FALSE, comment=NA, eval=FALSE}
woolf_texts = list()
# downloading all books
for( i in 1:nrow(woolf_list)){
  book = gutenberg_download(woolf_list[i, 1, 1])
  woolf_texts[[i]] = book
}
saveRDS(woolf_texts, file="data/woolf.rds")
```

```{r, message=FALSE, warning=FALSE, comment=NA}
woolf_texts = readRDS(file="data/woolf.rds")

woolf_uni_data = list()
woolf_bi_data = list()
for( i in 1:nrow(woolf_list)){
  book = woolf_texts[[i]]
  wbook = book %>% mutate(chapter = cumsum(str_detect(text, regex("chapter [\\divxlc]+", ignore_case = TRUE)))) %>%
    filter(text != "") %>% 
    group_by(chapter)
  
  bi = wbook %>% unnest_tokens(bigram, text, token = "ngrams", n = 2) %>% 
    separate(bigram, c("first_word", "second_word"), sep = " ") %>%
    count(first_word, second_word, sort = TRUE) %>% 
    mutate(collocation = paste(first_word, second_word, sep=" "),
           chapter_words = sum(n)) %>% 
    mutate(index = row_number(), tf = n/chapter_words) %>% 
    mutate(book = woolf_list[i,2,1])
  woolf_bi_data[[i]] = bi
    
  uni = wbook %>% unnest_tokens(unigram, text, token = "ngrams", n = 1) %>% 
    count(unigram , sort = TRUE) %>% 
    mutate(chapter_words = sum(n), index = row_number(),
           tf = n/chapter_words) %>% 
    mutate(book = woolf_list[i,2,1])
  woolf_uni_data[[i]] = uni
}

woolf_unigram_data_all = bind_rows(woolf_uni_data) %>% as.data.frame(stringsAsFactors = F)
woolf_unigram_data_all <- woolf_unigram_data_all %>% ungroup() %>% 
  group_by(book, chapter)

co = lm(log10(tf) ~ log10(index), data = woolf_unigram_data_all)
p = woolf_unigram_data_all %>% 
  ggplot(aes(index, tf, color = book)) + 
  geom_abline(intercept = co$coefficients[1], slope = co$coefficients[2], color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 1) + 
  ylab("term frequency") +
  ggtitle("1-gram distribution for Jane Austen") +
  scale_x_log10() +
  scale_y_log10()
p

woolf_bigram_data_all = bind_rows(woolf_bi_data) %>% as.data.frame(stringsAsFactors = F)
woolf_bigram_data_all <- woolf_bigram_data_all %>% ungroup() %>% 
  group_by(book, chapter)

co = lm(log10(tf) ~ log10(index), data = woolf_bigram_data_all)
p = woolf_bigram_data_all %>% 
  ggplot(aes(index, tf, color = book)) + 
  geom_abline(intercept = co$coefficients[1], slope = co$coefficients[2], color = "gray50", linetype = 2) +
  geom_line(size = 1.1, alpha = 1) + 
  ylab("term frequency") +
  ggtitle("2-gram distribution for Jane Austen") +
  scale_x_log10() +
  scale_y_log10()
p
```

<p dir="RTL"> 
بر اساس نمودار های فوق مشاهده می کنیم که آثار جین آستین نیز با یکدیگر شباهت داشته و با آثار دیکنز تفاوت دارد. البته مشاهده می کنیم که اثری که جین آستین در دوران نوجوانی نوشته است، تفاوت محسوسی با سایر آثار او دارد.
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
kruskal.test(book ~ tf, woolf_unigram_data_all)

kruskal.test(book ~ tf, woolf_bigram_data_all)
```

<p dir="RTL">
برای تست تفاوت میان کتاب های مختلف، از تست
kruskal 
استفاده می کنیم، اما همانطور که در نمودار نیز مشاهده کردیم، کتاب ها کاملا با یکدیگر مطابق نبوده و در نتیجه این موضوع باعث می شود که فرض صفر ما که کتاب ها از یک توزیع آمده اند، باطل شود.
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
wilcox.test(unigram_data_all$tf, woolf_unigram_data_all$tf,alternative = "two.sided", exact = FALSE, correct = FALSE)

wilcox.test(bigram_data_all$tf, woolf_bigram_data_all$tf,alternative = "two.sided", exact = FALSE, correct = FALSE)
```

<p dir="RTL"> 
همینطور برای مشاهده ی یکسان بودن توزیع آثار دو نویسنده، از تست
Wilcoxon–Mann–Whitney rank-sum test 
استفاده می کنیم، و مشاهده می کنیم که نتیجه نشان می دهد که فرض صفر باطل شده و دو نویسنده، دارای توزیع یکسانی نیستند.
</p>


***

<p dir="RTL"> 
۱۰. بر اساس دادهایی که در تمرین ۸ و ۹ از آثار دو نویسنده به دست آوردید و با استفاده از  N-gram ها یک مدل لاجستیک برای تشخیص صاحب اثر بسازید. خطای مدل چقدر است؟ برای یادگیری مدل از کتاب کتاب الیور تویست اثر دیکنز و کتاب پیرمرد و دریا استفاده نکنید. پس از ساختن مدل برای تست کردن فصل های این کتابها را به عنوان داده ورودی به مدل بدهید. خطای تشخیص چقدر است؟
</p>

```{r, message=FALSE, warning=FALSE, comment=NA}
dickens_data_all <- bind_rows(unigram_data_all %>% ungroup() %>% mutate(author = 0) %>% select(book, chapter, author, tf, index, n),
                              bigram_data_all %>% ungroup() %>% mutate(author = 0) %>% select(book, chapter, author, tf, index, n))

austen_data_all <- bind_rows(woolf_unigram_data_all %>% ungroup() %>% mutate(author = 1) %>% select(book, chapter, author, tf, index, n),
                              woolf_bigram_data_all %>% ungroup() %>% mutate(author = 1) %>% select(book, chapter, author, tf, index, n))

dickens_train<- dickens_data_all %>% filter(book != "Oliver Twist")
dickens_test <- dickens_data_all %>% filter(book == "Oliver Twist")

austen_train <- austen_data_all %>% filter(book != "Mansfield Park")
austen_test <- austen_data_all %>% filter(book == "Mansfield Park")

train <- bind_rows(dickens_train, austen_train)
test <- bind_rows(dickens_test, austen_test)

glm_model <- glm(author ~ tf + index + n + chapter,family = binomial(link = 'logit'), data = train)
summary(glm_model)

cutoff = 0.2
test$predict = predict(glm_model, newdata = test, type = 'response')
test <- test %>% mutate(get = ifelse(predict < cutoff, 0, 1))

P <- test %>% filter(author == 1) %>% nrow()
N <- test %>% filter(author == 0) %>% nrow()
TP <- test %>% filter(author == 1 & get == 1) %>% nrow()
TN <- test %>% filter(author == 0 & get == 0) %>% nrow()
FP <- test %>% filter(author == 0 & get == 1) %>% nrow()
FN <- test %>% filter(author == 1 & get == 0) %>% nrow()

ACC <- (TP + TN)/(P + N)
cat("Accuracy: ", ACC)

FPR <- 1 - (TN/N)
cat("False Positive Rate: ", FPR)

TPR <- TP/P
cat("True Positive Rate: ", TPR)
```

<p dir="RTL"> 
بر اساس مقالات خوانده شده، تشخیص نویسنده ی کتاب بر اساس این روش حدود ۷۰ درصد است. اما ما مدل خود را بر اساس
term frequency 
و 
rank 
و 
frequency 
و فصل کتاب، لرن کردیم. نتیجه مدل نشان داد که تمامی این متغیرها در مدل تاثیر قابل توجهی دارند. اما در نهایت میزان خطای ما حول ۰.۵ است و نتایج نشان می دهد که مدل ما بسیار شبیه مدل رندم عمل می کند. هم چنین در نهایت بدست آوردیم که دقت تشخیص ما ۵۲ درصد است.
</p>